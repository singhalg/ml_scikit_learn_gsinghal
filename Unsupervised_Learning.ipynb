{
 "metadata": {
  "name": "Unsupervised_Learning"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": "Unsupervised Learning: Dimensionality Reduction, Clustering and Visualization"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Previously we worked on visualizing the iris data by plotting\npairs of dimensions by trial and error, until we arrived at\nthe best pair of dimensions for our dataset.  Here we will\nuse an unsupervised *dimensionality reduction* algorithm\nto accomplish this more automatically."
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "By the end of this section you will\n\n- Know how to instantiate and train an unsupervised dimensionality reduction algorithm:\n  Principal Component Analysis (PCA)\n- Know how to use PCA to visualize high-dimensional data"
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": "Dimensionality Reduction: PCA"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Dimensionality reduction is the task of deriving a set of new\nartificial features that is smaller than the original feature\nset while retaining most of the variance of the original data.\nHere we'll use a common but powerful dimensionality reduction\ntechnique called Principal Component Analysis (PCA).\nWe'll perform PCA on the iris dataset that we saw before:"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "from sklearn.datasets import load_iris\niris = load_iris()\nX = iris.data\ny = iris.target",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "PCA is performed using linear combinations of the original features\nusing a truncated Singular Value Decomposition of the matrix X so\nas to project the data onto a base of the top singular vectors.\nIf the number of retained components is 2 or 3, PCA can be used\nto visualize the dataset."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "from sklearn.decomposition import PCA\npca = PCA(n_components=2, whiten=True)\npca.fit(X)",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Once fitted, the pca model exposes the singular vectors in the components_ attribute:"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "pca.components_",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Other attributes are available as well:"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "pca.explained_variance_ratio_",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "pca.explained_variance_ratio_.sum()",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Let us project the iris dataset along those first two dimensions:"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "X_pca = pca.transform(X)\n\nprint X[:5]\nprint X_pca[:5]",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "PCA `normalizes` and `whitens` the data, which means that the data\nis now centered on both components with unit variance:"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "import numpy as np\nnp.round(X_pca.mean(axis=0), decimals=5)",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "np.round(X_pca.std(axis=0), decimals=5)",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Furthermore, the samples components do no longer carry any linear correlation:"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "np.corrcoef(X_pca.T)",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "We can visualize the projection using pylab, but first\nlet's make sure our ipython notebook is in pylab inline mode"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "%pylab inline",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Now we can visualize the results using the following utility function:"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "import pylab as pl\nfrom itertools import cycle\n\ndef plot_2D(data, target, target_names):\n    colors = cycle('rgbcmykw')\n    target_ids = range(len(target_names))\n    pl.figure()\n    for i, c, label in zip(target_ids, colors, target_names):\n        pl.scatter(data[target == i, 0], data[target == i, 1],\n                   c=c, label=label)\n    pl.legend()",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Now calling this function for our data, we see the plot:"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "plot_2D(X_pca, iris.target, iris.target_names)",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Note that this projection was determined *without* any information about the\nlabels (represented by the colors): this is the sense in which the learning\nis **unsupervised**.  Nevertheless, we see that the projection gives us insight\ninto the distribution of the different flowers in parameter space: notably,\n*iris setosa* is much more distinct than the other two species."
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Note also that the default implementation of PCA computes the\nsingular value decomposition (SVD) of the full\ndata matrix, which is not scalable when both ``n_samples`` and\n``n_features`` are big (more that a few thousands).\nIf you are interested in a number of components that is much\nsmaller than both ``n_samples`` and ``n_features``, consider using\n`sklearn.decomposition.RandomizedPCA` instead."
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Other dimensionality reduction techniques which are useful to know about:\n\n- `sklearn.decomposition.PCA`: <br/>\n   Principal Component Analysis\n- `sklearn.decomposition.RandomizedPCA`:<br/>\n   fast non-exact PCA implementation based on a randomized algorithm\n- `sklearn.decomposition.SparsePCA`:<br/>\n   PCA variant including L1 penalty for sparsity\n- `sklearn.decomposition.FastICA`:<br/>\n   Independent Component Analysis\n- `sklearn.decomposition.NMF`:<br/>\n   non-negative matrix factorization\n- `sklearn.manifold.LocallyLinearEmbedding`: <br/>\n   nonlinear manifold learning technique based on local neighborhood geometry\n- `sklearn.manifold.IsoMap`: <br/>\n   nonlinear manifold learning technique based on a sparse graph algorithm"
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": "Clustering of Iris Data"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Clustering is the task of gathering samples into groups of similar\nsamples according to some predefined similarity or dissimilarity\nmeasure (such as the Euclidean distance).\nIn this section we will explore a basic clustering task on the\niris data.\n\nNow we will use one of the simplest clustering algorithms, K-means.\nThis is an iterative algorithm which searches for three cluster\ncenters such that the distance from each point to its cluster is\nminimizied. First, let's step back for a second,\nlook at the above plot, and think about what this will do.\nThe algorithm will look for three cluster centers, and label the\npoints according to which cluster center they're closest to."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "from sklearn.cluster import KMeans\nfrom numpy.random import RandomState\nrng = RandomState(42)\n\nkmeans = KMeans(n_clusters=3, random_state=rng)\nkmeans.fit(X)",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "KMeans can fall into local minima, and ``n_init`` can be used to cover optimization space more exhaustively. "
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "import numpy as np\nnp.round(kmeans.cluster_centers_, decimals=2)",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "The labels_ attribute of the K means estimator contains the ID of the cluster that each point is assigned to."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "kmeans.labels_",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "kmeans.inertia_",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "``inertia`` is the sum of squared distances to the closest centroid for all observations in the training set. \n\n**Question**: In which case do you get lower inertia? ``X`` or `X_pca`` ? \n\n**Question**: What effect do you expect to see on ``intertia`` by increasing the number of cluster centers ?  "
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "The K-means algorithm has been used to infer cluster labels for the points. Let's call the plot_2D function again, but color the points based on the cluster labels rather than the iris species."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "plot_2D(X_pca, kmeans.labels_, [\"c0\", \"c1\", \"c2\"])\nplt.title('K-Means labels')\n\nplot_2D(X_pca, iris.target, iris.target_names)\nplt.title('True labels')",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "cluster_dist = kmeans.transform(X)",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "The above method transforms X to a cluster-distance space.\nIn the new space, each dimension is the distance to the cluster centers. Please take a moment to think what \"cluster-distance space\" might mean."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "print cluster_dist.shape\nlabels = np.argmin(cluster_dist,1)\n#print cluster_dist\n",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "**Try** the ``fit`` method with both ``X`` and ``X_pca``."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "print labels",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "plot_2D(X_pca, labels, [\"c0\", \"c1\", \"c2\"])\nplt.title('K-Means labels')",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": "Some Notable Clustering Routines"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "The following are two well-known clustering algorithms. Like most unsupervised learning\nmodels in the scikit, they expect the data to be clustered to have the shape `(n_samples, n_features)`:\n\n- `sklearn.cluster.KMeans`: <br/>\n    The simplest, yet effective clustering algorithm. Needs to be provided with the\n    number of clusters in advance, and assumes that the data is normalized as input\n    (but use a PCA model as preprocessor).\n- `sklearn.cluster.MeanShift`: <br/>\n    Can find better looking clusters than KMeans but is not scalable to high number of samples.\n- `sklearn.cluster.DBSCAN`: <br/>\n    Can detect irregularly shaped clusters based on density, i.e. sparse regions in\n    the input space are likely to become inter-cluster boundaries. Can also detect\n    outliers (samples that are not part of a cluster).\n\nOther clustering algorithms do not work with a data array of shape (n_samples, n_features)\nbut directly with a precomputed affinity matrix of shape (n_samples, n_samples):\n\n- `sklearn.cluster.AffinityPropagation`: <br/>\n    Clustering algorithm based on message passing between data points.\n- `sklearn.cluster.SpectralClustering`: <br/>\n    KMeans applied to a projection of the normalized graph Laplacian: finds\n    normalized graph cuts if the affinity matrix is interpreted as an adjacency matrix of a graph.\n- `sklearn.cluster.Ward`: <br/>\n    Ward implements hierarchical clustering based on the Ward algorithm,\n    a variance-minimizing approach. At each step, it minimizes the sum of\n    squared differences within all clusters (inertia criterion).\n- `sklearn.cluster.DBSCAN`: <br/>\n    DBSCAN can work with either an array of samples or an affinity matrix."
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": "Imputation using Nearest Neighbor (Instance Based Learning)"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Neighbors-based classification is a type of instance-based learning or non-generalizing learning: it does not attempt to construct a general internal model, but simply stores instances of the training data. Classification is computed from a simple majority vote of the nearest neighbors of each point: a query point is assigned the data class which has the most representatives within the nearest neighbors of the point."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "n = 0.05 #let's decide to throw away 5 % of data points.\nnum_missing_vals = int(n*iris.data.shape[0]*iris.data.shape[1])\nprint num_missing_vals\n",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "import copy\nlossy_X = copy.deepcopy(X)\nmissing_X = copy.deepcopy(X) # this is exactly similar to lossy_X, with NaNs replaced by 0 so that we can use this to build the Nearest Neighbor learner. \nlost_data_locations = []\nfor i in range(num_missing_vals):\n    a = random.randint(0,149)\n    b = random.randint(0,3)\n    lost_data_locations.append([a,b])\n    lossy_X[a][b] = np.nan # at a randomly chosen location, we replace the real value with NaN\n    missing_X[a][b] = 0   # at a randomly chosen location, we replace the real value with 0, we'll use missing_X to build the NN learner. \n\n#print lost_data_locations",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "print lossy_X[:30]",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "from sklearn.neighbors import NearestNeighbors as NN\nNearestNeighbor = NN(n_neighbors=5, radius=1.0)",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "NearestNeighbor.fit(missing_X)",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def returnImputedValue(lossy_X, neighbors, feature_index):\n    ret_val = 0\n    ret_val_deno = 0\n    for each in neighbors:\n        if np.any(np.isnan(lossy_X[each][feature_index]))==False:\n            ret_val+=lossy_X[each][feature_index]\n            ret_val_deno+=1\n    return float(ret_val)/ret_val_deno",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "imputed_X = copy.deepcopy(lossy_X)\nimport numpy as np\nprint iris.data.shape[0]\nprint iris.data.shape[1]\nfor i in range(150):\n    for j in range(4):\n        if np.isnan(lossy_X[i][j]):\n            k_neighbors = NearestNeighbor.kneighbors(X[i], return_distance=False)\n            imputed_X[i][j]=returnImputedValue(lossy_X, k_neighbors[0],j)\n            \n            \nprint imputed_X[:30]\n\nfor each in lost_data_locations:\n    print X[each[0],each[1]], '      ', imputed_X[each[0],each[1]]\n            ",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "",
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}