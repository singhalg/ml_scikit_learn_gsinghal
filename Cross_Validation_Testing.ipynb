{
 "metadata": {
  "name": "Cross_Validation_Testing"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "raw",
     "metadata": {},
     "source": "This notebook is an adaptation of Jake Vanderplas' tutorial on scikit-learn, published on Mar 14, 2013.  "
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": "Measuring Classification Performance: Validation & Testing"
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": "Checking Performance on the Iris Dataset"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Previously, we looked at a simplistic example of how to test the performance\nof a classifier.  Using the iris data set, it looked something like this:"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "import os\nprint os.getcwd()\nos.chdir('C:\\Users\\Dong Yu\\Desktop\\ML_Lectures') # your current working directory\nos.getcwd()\n\n\n# Get the data\nfrom sklearn.datasets import load_iris\niris = load_iris()\nX = iris.data\ny = iris.target",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# Instantiate and train the classifier\nfrom sklearn.svm import LinearSVC\nclf = LinearSVC(loss = 'l2')\nclf.fit(X, y)",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# Check input vs. output labels\ny_pred = clf.predict(X)\nprint (y_pred == y)",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "**Question:** what might be the problem with this approach?"
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": "A Better Approach: Cross-Validation"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Learning the parameters of a prediction function and testing it on the\nsame data is a methodological mistake: a model that would just repeat\nthe labels of the samples that it has just seen would have a perfect\nscore but would fail to predict anything useful on yet-unseen data. This situation is called **over-fitting.**\nTo avoid it, it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a test set ``X_test, y_test``, and build/fit/train the model using the remaining **training set**.\n\nThus, to avoid over-fitting, we have to define two different sets:\n\n- a training set X_train, y_train which is used for learning the parameters of a predictive model\n- a testing set X_test, y_test which is used for evaluating the fitted predictive model\n\nIn scikit-learn such a random split can be quickly computed with the\n`train_test_split` helper function.  It can be used this way:"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "from sklearn import cross_validation\n\nX_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.10, random_state=0)\n\nprint X.shape, X_train.shape, X_test.shape",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Now we train on the training data, and test on the testing data:"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "clf = LinearSVC(loss='l2').fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nprint (y_pred == y_test)\n\n# calculating accuracy\naccuracy = 0\nfor i in range(len(y_pred)):\n    if y_pred[i]==y_test[i]:\n        accuracy+=1\n\nprint accuracy*100/float(len(y_pred))",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "**Try** decreasing the ``test_size`` from ``0.5`` to ``0.1`` . \n\nThere is an issue here, however:\nby defining these two sets, we drastically reduce the number\nof samples which can be used for learning the model, and the results\ncan depend on a particular random choice for the pair of (train, test) sets.\n\nA solution is to split the whole data several consecutive times in different\ntrain set and test set, and to return the averaged value of the prediction\nscores obtained with the different sets. Such a procedure is called **cross-validation**.\nThis approach can be computationally expensive, but does not waste too much data\n(as it is the case when fixing an arbitrary test set), which is a major advantage\nin problem such as inverse inference where the number of samples is very small.\n\nWe'll explore cross-validation a bit later, but\nyou can find much more information on cross-validation in scikit-learn here:\nhttp://scikit-learn.org/dev/modules/cross_validation.html\n"
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": "Diving Deeper: Hyperparameters, Over-fitting, and Under-fitting"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "*The content in this section is adapted from Andrew Ng's excellent\nCoursera course, available here:* https://www.coursera.org/course/ml"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "The issues associated with validation and \ncross-validation are some of the most important\naspects of the practice of machine learning.  Selecting the optimal model\nfor your data is vital, and is a piece of the problem that is not often\nappreciated by machine learning practitioners.\n\nOf core importance is the following question:\n\n**If our estimator is underperforming, how should we move forward?**\n\n- Use simpler or more complicated model?\n- Add more features to each observed data point?\n- Add more training samples?\n\nThe answer is often counter-intuitive.  In particular, **Sometimes using a\nmore complicated model will give _worse_ results.**  Also, **Sometimes adding\ntraining data will not improve your results.**  The ability to determine\nwhat steps will improve your model is what separates the successful machine\nlearning practitioners from the unsuccessful."
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": "A Simple Regression Problem"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "For this section, we'll work with a simple 1D regression problem.  This will help us to\neasily visualize the data and the model, and the results generalize easily to  higher-dimensional\ndatasets.  We'll explore **polynomial regression**: the fitting of a polynomial to points.\nThough this can be accomplished within scikit-learn (the machinery is in `sklearn.linear_model`),\nfor simplicity we'll use `numpy.polyfit` and `numpy.polyval`:"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "%pylab inline",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "import numpy as np\n\nx = 10 * np.random.random(20)\ny = 0.5 * x ** 2 - x + 1\n\np = np.polyfit(x, y, deg=2)\nprint p",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "As we can see, polyfit fits a polynomial to one-dimensional data. We can\nvisualize this to see the result:"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "x_new = np.linspace(-1, 12, 1000)\ny_new = np.polyval(p, x_new)\n\nplt.scatter(x, y)\nplt.plot(x_new, y_new)",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "We've chosen the model to use through the *hyperparameter* `deg`.\n\nA *hyperparameter* is a parameter that determines the type of\nmodel we use: for example, `deg=1` gives a linear model, `deg=2`\ngives a 2nd-order polynomial, etc."
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": "Adding some noise"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Now, what if the data is not a perfect polynomial?  Below, we'll take the above\nproblem and add a small\namount of Gaussian scatter in ``y``.  Here we'll take the additional step of computing\nthe RMS error of the resulting model on the input data."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "np.random.seed(42)\nx = 10 * np.random.random(20)\ny = 0.5 * x ** 2 - x + 1  + np.random.normal(0, 2, x.shape)\n\n# np.random.normal(0, 2, x.shape) is the error term\n\n# ---> Change the degree here\np = np.polyfit(x, y, deg=2)\nx_new = np.linspace(0, 10, 100)\ny_new = np.polyval(p, x_new)\n\nplt.scatter(x, y)\nplt.plot(x_new, y_new)\nplt.ylim(-10, 50)\nprint \"RMS error = %.4g\" % np.sqrt(np.mean((y - np.polyval(p, x)) ** 2))",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "**What happens to the fit and the RMS error as the degree is increased?**"
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": "Learning Curves and the Bias/Variance Tradeoff"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "One way to address this issue is to use what are often called **Learning Curves**.\nGiven a particular dataset and a model we'd like to fit (e.g. a polynomial), we'd\nlike to tune our value of the *hyperparameter* `d` to give us the best fit.\n\nWe'll imagine we have a simple regression problem: given the size of a house, we'd\nlike to predict how much it's worth.  We'll fit it with our polynomial regression\nmodel.\n\nRun the following code to see an example plot:"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "import os\nos.chdir('C:\\Users\\Dong Yu\\Desktop\\ML_Lectures') \n# your current working directory\nos.getcwd()\n\nimport plot_bias_variance_examples\n\nplot_bias_variance_examples.plot_bias_variance(8)\n",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "In the above figure, we see fits for three different values of `d`.\nFor `d = 1`, the data is under-fit. This means that the model is too\nsimplistic: no straight line will ever be a good fit to this data. In\nthis case, we say that the model suffers from high bias. The model\nitself is biased, and this will be reflected in the fact that the data\nis poorly fit. At the other extreme, for `d = 6` the data is over-fit.\nThis means that the model has too many free parameters (6 in this case)\nwhich can be adjusted to perfectly fit the training data. If we add a\nnew point to this plot, though, chances are it will be very far from\nthe curve representing the degree-6 fit. In this case, we say that the\nmodel suffers from high variance. The reason for this label is that if\nany of the input points are varied slightly, it could result in an\nextremely different model.\n\nIf we have too many features, the learned hypothesis \nmay fit the training set very well, but fail \nto generalize to new examples (predict prices on new examples)\n\nIn the middle, for `d = 2`, we have found a good mid-point. It fits\nthe data fairly well, and does not suffer from the bias and variance\nproblems seen in the figures on either side. What we would like is a\nway to quantitatively identify bias and variance, and optimize the\nmetaparameters (in this case, the polynomial degree d) in order to\ndetermine the best algorithm. This can be done through a process\ncalled cross-validation."
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": "Validation Curves"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "We'll create a dataset like in the example above, and use this to test our\nvalidation scheme.  First we'll define some utility routines:"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def test_func(x, err=0.5):\n    return np.random.normal(10 - 1. / (x + 0.1), err)\n\ndef compute_error(x, y, p):\n    yfit = np.polyval(p, x)\n    return np.sqrt(np.mean((y - yfit) ** 2))",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "from sklearn.cross_validation import train_test_split\n\nN = 200\nf_crossval = 0.5\nerror = 1.0\n\n# randomly sample the data\nnp.random.seed(1)\nx = np.random.random(N)\ny = test_func(x, error)\n\n# split into training, validation, and testing sets.\nxtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=f_crossval)\n\n# show the training and cross-validation sets\nplt.scatter(xtrain, ytrain, color='red')\nplt.scatter(xtest, ytest, color='blue')",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "In order to quantify the effects of bias and variance and construct\nthe best possible estimator, we will split our training data into\na *training set* and a *validation set*.  As a general rule, the\ntraining set should be about 60% of the samples.\n\nThe general idea is as follows. The model parameters (in our case,\nthe coefficients of the polynomials) are learned using the training\nset as above. The error is evaluated on the cross-validation set,\nand the meta-parameters (in our case, the degree of the polynomial)\nare adjusted so that this cross-validation error is minimized.\nFinally, the labels are predicted for the test set. These labels\nare used to evaluate how well the algorithm can be expected to\nperform on unlabeled data.\n\nThe cross-validation error of our polynomial classifier can be visualized\nby plotting the error as a function of the polynomial degree d. We can do\nthis as follows:"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# suppress warnings from Polyfit\nimport warnings\nwarnings.filterwarnings('ignore', message='Polyfit*')\n\ndegrees = np.arange(21)\ntrain_err = np.zeros(len(degrees))\nvalidation_err = np.zeros(len(degrees))\n\nfor i, d in enumerate(degrees):\n    p = np.polyfit(xtrain, ytrain, d)\n\n    train_err[i] = compute_error(xtrain, ytrain, p)\n    validation_err[i] = compute_error(xtest, ytest, p)\n\nfig, ax = plt.subplots()\n\nax.plot(degrees, validation_err, lw=2, label = 'cross-validation error')\nax.plot(degrees, train_err, lw=2, label = 'training error')\nax.plot([0, 20], [error, error], '--k', label='intrinsic error')\n\nax.legend(loc=0)\nax.set_xlabel('degree of fit')\nax.set_ylabel('rms error')",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "This figure compactly shows the reason that cross-validation is\nimportant. On the left side of the plot, we have very low-degree\npolynomial, which under-fits the data. This leads to a very high\nerror for both the training set and the cross-validation set. On\nthe far right side of the plot, we have a very high degree\npolynomial, which over-fits the data. This can be seen in the fact\nthat the training error is very low, while the cross-validation\nerror is very high. Plotted for comparison is the intrinsic error\n(this is the scatter artificially added to the data: click on the\nabove image to see the source code). For this toy dataset,\nerror = 1.0 is the best we can hope to attain. Choosing `d=6` in\nthis case gets us very close to the optimal error.\n\nThe astute reader will realize that something is amiss here: in\nthe above plot, `d = 6` gives the best results. But in the previous\nplot, we found that `d = 6` vastly over-fits the data. What\u2019s going\non here? The difference is the **number of training points** used.\nIn the previous example, there were only eight training points.\nIn this example, we have 100. As a general rule of thumb, the more\ntraining points used, the more complicated model can be used.\nBut how can you determine for a given model whether more training\npoints will be helpful? A useful diagnostic for this are learning curves."
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": "Learning Curves"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "A learning curve is a plot of the training and cross-validation\nerror as a function of the number of training points. Note that\nwhen we train on a small subset of the training data, the training\nerror is computed using this subset, not the full training set.\nThese plots can give a quantitative view into how beneficial it\nwill be to add training samples."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# suppress warnings from Polyfit\nimport warnings\nwarnings.filterwarnings('ignore', message='Polyfit*')\n\ndef plot_learning_curve(d):\n    sizes = np.linspace(2, N, 50).astype(int)\n    train_err = np.zeros(sizes.shape)\n    crossval_err = np.zeros(sizes.shape)\n\n    for i, size in enumerate(sizes):\n        # Train on only the first `size` points\n        p = np.polyfit(xtrain[:size], ytrain[:size], d)\n        \n        # Validation error is on the *entire* validation set\n        crossval_err[i] = compute_error(xtest, ytest, p)\n        \n        # Training error is on only the points used for training\n        train_err[i] = compute_error(xtrain[:size], ytrain[:size], p)\n\n    fig, ax = plt.subplots()\n    ax.plot(sizes, crossval_err, lw=2, label='validation error')\n    ax.plot(sizes, train_err, lw=2, label='training error')\n    ax.plot([0, N], [error, error], '--k', label='intrinsic error')\n\n    ax.set_xlabel('traning set size')\n    ax.set_ylabel('rms error')\n    \n    ax.legend(loc=0)\n    \n    ax.set_xlim(0, 99)\n\n    ax.set_title('d = %i' % d)",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Now that we've defined this function, let's plot an example learning curve:"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "plot_learning_curve(d=4)",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Here we show the learning curve for `d = 1`. From the above\ndiscussion, we know that `d = 1` is a high-bias estimator which\nunder-fits the data. This is indicated by the fact that both the\ntraining and validation errors are very high. If this is\nthe case, adding more training data will not help matters: both\nlines have converged to a relatively high error.\n\n**When the learning curves have converged, we need a more sophisticated\nmodel or more features to improve the error.**\n\n*(equivalently we can decrease regularization, which we won't discuss in this tutorial)*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "plot_learning_curve(d=20)\nplt.ylim(0, 15)",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Here we show the learning curve for `d = 20`. From the above\ndiscussion, we know that `d = 20` is a high-variance estimator\nwhich over-fits the data. This is indicated by the fact that the\ntraining error is much less than the validation error. As\nwe add more samples to this training set, the training error will\ncontinue to climb, while the cross-validation error will continue\nto decrease, until they meet in the middle. In this case, our\nintrinsic error was set to 1.0, and we can infer that adding more\ndata will allow the estimator to very closely match the best\npossible cross-validation error.\n\n**When the learning curves have not converged, it indicates that the\nmodel is too complicated for the amount of data we have.  We should\neither find more training data, or use a simpler model.**\n\n*(equivalently we can increase __regularization__, which we won't discuss in this tutorial)*"
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": "Summary"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "We\u2019ve seen above that an under-performing algorithm can be due\nto two possible situations: high bias (under-fitting) and high\nvariance (over-fitting). In order to evaluate our algorithm, we\nset aside a portion of our training data for cross-validation.\nUsing the technique of learning curves, we can train on progressively\nlarger subsets of the data, evaluating the training error and\ncross-validation error to determine whether our algorithm has\nhigh variance or high bias. But what do we do with this information?"
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": "High Bias"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "If our algorithm shows high **bias**, the following actions might help:\n\n- **Add more features**. In our example of predicting home prices,\n  it may be helpful to make use of information such as the neighborhood\n  the house is in, the year the house was built, the size of the lot, etc.\n  Adding these features to the training and test sets can improve\n  a high-bias estimator\n- **Use a more sophisticated model**. Adding complexity to the model can\n  help improve on bias. For a polynomial fit, this can be accomplished\n  by increasing the degree d. Each learning technique has its own\n  methods of adding complexity.\n- **Use fewer samples**. Though this will not improve the classification,\n  a high-bias algorithm can attain nearly the same error with a smaller\n  training sample. For algorithms which are computationally expensive,\n  reducing the training sample size can lead to very large improvements\n  in speed.\n- **Decrease regularization**. Regularization is a technique used to impose\n  simplicity in some machine learning models, by adding a penalty term that\n  depends on the characteristics of the parameters. If a model has high bias,\n  decreasing the effect of regularization can lead to better results."
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": "High Variance"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "If our algorithm shows **high variance**, the following actions might help:\n\n- **Use fewer features**. Using a feature selection technique may be\n  useful, and decrease the over-fitting of the estimator.\n- **Use more training samples**. Adding training samples can reduce\n  the effect of over-fitting, and lead to improvements in a high\n  variance estimator.\n- **Increase Regularization**. Regularization is designed to prevent\n  over-fitting. In a high-variance model, increasing regularization\n  can lead to better results.\n\nThese choices become very important in real-world situations. For example,\ndue to limited telescope time, astronomers must seek a balance between\nobserving a large number of objects, and observing a large number of\nfeatures for each object. Determining which is more important for a\nparticular learning task can inform the observing strategy that the\nastronomer employs. In a later exercise, we will explore the use of\nlearning curves for the photometric redshift problem."
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": "More Sophisticated Methods"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "There are a lot more options for performing validation and model testing.\nIn particular, there are several schemes for cross-validation, in which\nthe model is fit multiple times with different training and test sets.\nThe details are different, but the principles are the same as what we've\nseen here.\n\nFor more information see the ``sklearn.cross_validation`` module documentation,\nand the information on the scikit-learn website."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "import os\nos.chdir('C:\\Users\\Dong Yu\\Desktop\\ML_Lectures') \n# your current working directory\nos.getcwd()\n\nimport plot_roc_crossval\nplot_roc_crossval.stratified_cross_validation()",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": "One Last Caution"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Using validation schemes to determine hyper-parameters means that we are\nfitting the hyper-parameters to the particular validation set.  In the same\nway that parameters can be over-fit to the training set, hyperparameters can\nbe over-fit to the validation set.  Because of this, the validation error\ntends to under-predict the classification error of new data.\n\nFor this reason, it is recommended to split the data into three sets:\n\n- The **training set**, used to train the model (usually ~60% of the data)\n- The **validation set**, used to validate the model (usually ~20% of the data)\n- The **test set**, used to evaluate the expected error of the validated model (usually ~20% of the data)\n\nThis may seem excessive, and many machine learning practitioners ignore the need\nfor a test set.  But if your goal is to predict the error of a model on unknown\ndata, using a test set is vital."
    }
   ],
   "metadata": {}
  }
 ]
}